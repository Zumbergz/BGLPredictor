## Intro

Type 1 diabetes mellitus (T1DM) is an autoimmune disease that typically manifests itself in those between 10 and 14 years of age. That is not to say, however, that diagnosis of T1DM is restricted to those individuals within this age range, and whilst it is rare for diagnosis to be made in individuals over 40, it is possible for T1DM to occur at any age. T1DM is characterised by the destruction of the beta-cells within the pancreas which are responsible for producing insulin. This loss of insulin producing beta cells means that there is an inability for T1DM patients to regulate their own blood glucose levels, and leads to hyperglycemia (blood glucose levels being too high). As such, T1DM patients are reliant on external insulin to control the glycemic levels in their body. Diagnosis of the condition is based on hyperglycemia, with a fasting blood glucose concentration above 7.0 mmol/L being characterised as enough to diagnose. Other symptoms commonly seen before diagnosis are polyuria, polydipsia and weight loss.

According to the WHO, blood glucose levels should be between 3.9 mmol/L and 5.6mmol/L for ordinary people and between 5 mmol/L and 9 mmol/L for diabetic patients. As a result of this, there is a constant need for diabetics to manage their blood glucose levels, with the aim of spending as much time within this range as possible, and avoiding hyperglycemia and hypoglycemia (blood glucose levels too low). Both hyperglycemia and hypoglycemia are serious conditions and can lead to life threatening situations for patients; making precision when managing blood glucose levels a matter of life and death. Patients who spend extended periods of time in a hyperglycemic state can suffer from kidney damage, nerve damage, damage to the blood vessels of the retina, and complications with bones and joints. In extreme cases of hyperglycemia, diabetic ketoacidosis can occur, which, if left untreated, can lead to a life-threatening diabetic coma. Meanwhile, hypoglycemia affords a range of different issues, ranging from slurred speech and numbness to seizures, coma and death in extreme cases. 

With an estimated 422 million people worldwide suffering from diabetes, making it the 3rd most common chronic illness in the world, it is clear that there is a need and a great benefit to be gained from using technology to help patients more accurately manage their blood glucose levels. Unfortunately, regulating blood glucose levels is not an exact science, and there are around 48 different factors that can impact a patient's blood glucose levels. The predominant factor is carbohydrates ingested by the patient. Carbohydrates are broken down by the amylase enzyme into glucose which is transferred into the blood stream to be absorbed by cells; clearly having a direct impact on the levels of glucose in the blood. Other major factors include any physical activity done by the patient, and any external insulin taken by the patient. Whilst these factors are more easily measurable for patients, and thus can be adapted to and controlled, other factors such as stress, illness and injury, and hormonal changes can also have a substantial impact on blood glucose levels, whilst being much more difficult to manage. This makes the task of predicting how blood glucose levels will change and responding accordingly difficult for T1DM patients and machines alike - one small change can lead to large variations in behaviour.  

The end vision of diabetes related technology is known as the Artificial Pancreas (AP) and consists of a fully closed-loop system where a T1DM patient would have to do no management of their blood glucose level at all; the AP would handle everything. Unfortunately, due to the problems in variation and some other issues we will cover below, the creation of this AP system still seems to be far in the future. However, that is not to say that technology has had no impact on the lives of T1DM patients, as there have been many advances which have made noticeable impacts on improving the quality of life of patients. Introduced in 1981, insulin pens are devices that allow patients to inject insulin directly into their body. Coupled together with small, easy to use glucometers which were introduced in the 1980s, patients were able to frequently monitor their blood glucose levels, and respond accordingly by either using their insulin pens, or ingesting more glucose. This method of self-monitoring of Blood Glucose (SMBG) has significant drawbacks as blood is sampled intermittently, providing only short insights of glucose concentrations, whilst ignoring ongoing glucose fluctuations. With the idea of being able to monitor blood glucose at all times, Continuous Glucose Monitoring (CGM) devices were introduced that measures glucose concentrations at intervals (usually between 1-5 min), and then transmits/stores those values. Alongside the invention of the insulin pump, a device that allows programmable infusions of insulin over long, or short, periods of time, diabetic patients were given a much more reliable way of constantly monitoring, and regulating their blood glucose levels.

The creation of CGM devices also lead to a new capability - the ability to create blood glucose datasets by accesing the data stored in the CGM; datasets which are needed to train accurate Machine Learning (ML) models. It is clear that the capability to accurately predict when one might suffer a hyperglycemic, or hypoglycemic, episode would be invaluable to a T1DM patient, as it would allow them to behave proactively to prevent the episode from ever occurring, rather than the current reactionary method, which requires a hypo/hyperglycemic episode to be taking place before we can detect its presence. We name the amount of time into the future which we forecast the Prediction Horizon (PH). Current prediction horizons available to T1DM patients are very short (~5 mins? Find a stat), and thus don't give ample time for patients to react to predicted issues. As such, much work has been done in applying ML techniques to improve the length of the PH to a length of hours instead of minutes, but as of yet no concrete solution to the problem has been found. This is due to a variety of reasons, such as the variability of a patient's BG level due to uncontrollable factors day by day which was mentioned earlier, as well as some other issues that we will discuss later as we take a look at and disccuss different approaches that have been experimented with to solve this very problem.

## Main Bit
#### Intro
Machine learning is a field of Artificial Intelligence that focuses on learning through observing patterns in large quantities of past data, and iteratively improving accuracy at a given task. In the context of T1DM, the large quantities of past data refer to the patient's blood glucose levels over a period of months (as well as other points of interest such as exercise done / carbohydrate intake), whilst the task to improve at is predicting the BG levels of the patient for a specified PH. In particular we will look at Neural Networks (NN), a kind of Machine learning model that is able to learn very complex relationships between data by minimising the value of an error function, through iterative processes such as gradient descent. This error function can be specified to suit the model as needed and it is common, in the case of blood glucose forecasting, to see an error function comparing the absolute difference between forecasted blood glucose levels, and real recorded blood glucose levels. Much work has been done researching the applications of different kinds of NN to blood glucose forecasting, as it was believed that NNs would be able to solve the problem of intersubject variability - by training a new network on each individual's data. 

When training neural networks, there is much debate amongst researchers about what to include and what not to include within the input parameters to the network. Some believe that just the CGM data is sufficient, claiming that the inclusiuon of other measures, such as carbohydrate intake, would simply increase variance and decrease model performance. This is sharply contrasted by researchers who have experimented with both complex and simple physiological models to try and model the effect that factors such as exercise and insulin intake will have on future glucose levels. There is much debate about the correlation between past glucose levels and future levels, with some using only the current blood glucose levels as an input to the model, and others using many past readings to inform future predictions. Whilst there is much diversity amongst the methods employed, unfortunately, the overarching theme from most NN approaches is that they are very accurate at shorter PH (~30 minutes), but that accuracy quickly drops off at larger PH (~6 hours). 

### ANN
One such example of the application of NNs to the task of blood glucose prediction was by ((**Paper 3, insert ref**)), who attempted to use a NN trained on only CGM data. The model considered the preceding 20 minutes of CGM data, and attempted to predict BG levels at 3 different PHs of 15, 30 and 45 minutes. They would measure the accuracy of the model using the Root Mean Squared Error (RMSE) - a widely used metric for BG prediction models that takes the square root of the sum of the squares of all prediction errors - and the Prediction Delay (PD) - a metric that takes into account the delay between when certain features of the CGM data were seen in the original data and the predicted data (for instance the time gap between a peak appearing in the original and predicted data). When making a prediction, they compared two different approaches - their standard NN model that would make one prediction across the PH, and an AutoRegressive Model (ARM). In an ARM, the model makes the prediction one step at a time, and uses a recursive algorithm to allow each prediction to inform subsequent predictions, trying to capture the relationships between previous and future BG levels more accurately. They found that across the 15, 30 and 45 minute PHs there was a RMSE of ~10, 18, 27, mg/dL respectively. For upward trends in the data, there was a PD of 4, 9 and 14 minutes respectively, and for downward trends, a PD of 5, 15 and 26 minutes was seen. 

This use of a basic NN highlights some of the main problems with this approach when applied to blood glucose prediction - firstly that the accuracy falls of very quickly, with 2.7 times worse performance over a short difference in time of 30 minutes, and secondly that the model is much quicker at recognising upward than downward trends. This is a major issue, as downward trends lead to hypoglycemic episodes, which can be far more dangerous in the short term. To try and combat the first of these problems, ((**Paper 5 insert ref**)) included time-domain attributes along with their model. Time-domain features of the past CGM data such as minimum, maximum, mean, standard deviation etc. are passed as inputs to the model along with the data, in the hopes of improving accuracy. The approach was tested with the data of 12 different patients, across different PHs of 15, 30, 45 and 60 minutes. The accuracy found varied from patient to patient, with the best patient having RMSE values of 1.42, 3.61, 6.22 and 8.03 mg/dL respectively across the PHs and the worst obtaining values of 4.72, 11.32, 16.88 and 26.65 mg/dL respectively. Whilst this is a great improvement on the basic use of a NN, it highlights the large performance variability across patients - a variability that could result in medical emergencies if used medically currently.

#### RNN
Another method to try and better capture the relationship between previous CGM readings and future expected readings is a specific type of NN called a Recurrent Neural Network (RNN). An RNN differs from an ordinary 'feedforward' NN in the sense that in a NN, all inputs are passed from the input layer 'forwards' through each layer until they reach the output layer, the inputs are recursively passed into the network such that the value at a node in the network depends on both the current input and a hidden representation, which is calculated by all of the previous inputs to the model. In this way, RNNs offer a way to calculate compound representations that depend on all of the previous inputs to the network, rather than just passing in and handling the previous CGM data as separate datapoints. The most often seen implementation of RNNs is the Long Short Term Memory (LSTM) RNN, which implements an input, output and forget gate to control the value of the recursive hidden representation. This is done in such a way that it avoids the problems of exploding/vanishing gradient, which can affect training in vanilla RNNs. 

((**Paper 1 ref**)) Gavin Robertson et al. used RNNs to perform BG prediction on data generated from the AIDA diabetes simulator. AIDA is a freeware diabetes simulator that models steady-state glucose-insulin interactions by describing the physiology of a person with T1DM. This model takes into account carbohydrate intake, insulin, kidney function and insulin sensitivity. Predictions were made on the data for both short (15-60 minutes) and long term (8-10 hours) PHs. Some points of note viewed from their experiment were that in short term prediction, there were errors between the predicted and actual Blood Glucose Levels (BGL) of over 1 mmol/L - an amount that is very unsafe in practice as the different between 3 mmol/L and 2 mmol/L is potentially dangerous. It was also noted that predictions directly after input events (e.g. meal eaten, insulin taken) were much less accurate; showing that the model wasn't able to correctly handle these events as it should. The model was also seen to be far more accurate at night, suspectedly due to the lack of events such as meals leading to skew in BGL that the model wasn't able to properly handle. Overall, in the short term a RMSE of $0.15 \pm 0.04$ SD mmol/L was viewed and in the long term a RMSE of $0.14 \pm 0.16$ SD mmol/L was seen - again we note the trends that the model is far more unpredictable at longer term PH. Whilst these results are very promising, and the accuracy viewed is very good, it is also important to consider that the data was obtained from a mathematical model, and so is not hugely indicative of performance on real life data. This is due to the fact that the mathematical model only handles a few variables to enact changes on its output BGLs, whereas there are many many more factors whose fluctuations can impact BGLs in real-life patient data.

As we saw in the approach taken by Gavin Robertson et al., their model performed worse around times where significant events took place such as the ingestion of carbs and insuling being dosed. This is a common issue in the research of BGL prediction and whilst some believe that solely by studying trends in the CGM data of a patient, we should be able to implicitly figure out when events like this take place, and change our predictions accordingly, there is a separate approach of using Compartmental Models (CMs) to directly input the data of these events into the network. CMs are mathematical functions that try and describe the processes that occur in the inaccessible parts of the human body. For instance you would have one model describing how the carbohydrate intake at a given time would impact blood glucose levels over a given period of time afterwards. These CMs can range from simple bell curves to much more complex models such as the Hovorka model (**Ref??**), and there has been research done into the benefits and drawbacks of using simple or more complex compartmental models. 

((**Paper 4 Ref**)) Mougiakakou et al. proposed a solution for Blood Glucose Forecasting involving the combination of CMs and RNNs. The input data is passed to three separate CMs, which estimate 1) the effect of short acting insulin intake, 2) the effect of fast acting insulin intake, and 3) the effect of carbohydrate intake on blood glucose levels. The ouputs of these three CMs are then passed to the RNN along with past CGM data in order to predict subsequent blood glucose levels. Another point of note in this approach is that the RNN is trained with a Real-Time Recurrent Learning (RTRL) algorithm. Ordinarily, when training a network, you optimise the weights with a learning algorithm on a set of training data, then use those pre-determined optimised weights whenever making a prediction. Conversely, with Real Time Learning (RTL), the weights of the network continue to be improved during use, and each pair of predicted and real BGL values is used to iteratively update the network in real time. This is thought be very applicable to BGL prediction, as you would be able to intially train a model on a batch of patient data, then continually improve the model as it was being used by the patient; permitting the model to become as personalised and accurate as possible for the user. 

#### CNN


